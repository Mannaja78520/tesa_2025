from sahi import AutoDetectionModel
from sahi.predict import get_sliced_prediction
import cv2
import os

# ---------- CONFIG ----------
DRONE_MODEL_PATH = "drone.pt"
TEST_DIR = "P1_DATASET/TEST_DATA"
SAVE_DIR = "P1_DATASET/TEST_RESULTS_SAHI"

DRONE_CLASS_NAME = "drone"
MAX_DRONES = 2

CONF_THRESH = 0.275
CONF_UNDER_LINE_THRESH = 0.70

SLICE_W = 320
SLICE_H = 320
OVERLAP = 0.2

ZOOM = 5.0
AUTO_DELAY_MS = 500

GROUND_RATIO = 0.65
BIG_OBJ_RATIO = 0.0

MIN_RATIO = 0.0
MAX_RATIO = 1.0
# -----------------------------


def mask_datetime_by_contour(img):
    """
    ‡∏•‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ö‡∏£‡∏¥‡πÄ‡∏ß‡∏ì‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏ß‡∏•‡∏≤ (‡∏°‡∏∏‡∏°‡∏Ç‡∏ß‡∏≤‡∏ö‡∏ô) ‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏≤‡∏£‡∏´‡∏≤ contour
    """
    H, W = img.shape[:2]

    # ----- 1) ‡∏Å‡∏≥‡∏´‡∏ô‡∏î ROI ‡∏°‡∏∏‡∏°‡∏Ç‡∏ß‡∏≤‡∏ö‡∏ô (‡∏õ‡∏£‡∏±‡∏ö‡πÄ‡∏õ‡∏≠‡∏£‡πå‡πÄ‡∏ã‡πá‡∏ô‡∏ï‡πå‡πÑ‡∏î‡πâ) -----
    roi_x1 = int(W * 0.55)
    roi_y1 = 0
    roi_x2 = W
    roi_y2 = int(H * 0.20)     # ‡πÄ‡∏≠‡∏≤‡πÅ‡∏Ñ‡πà 20% ‡∏ö‡∏ô‡∏™‡∏∏‡∏î‡∏û‡∏≠

    roi = img[roi_y1:roi_y2, roi_x1:roi_x2]
    if roi.size == 0:
        return img

    # ----- 2) ‡∏´‡∏≤ mask ‡∏Ç‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠‡∏™‡∏µ‡∏Ç‡∏≤‡∏ß -----
    gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
    # ‡πÄ‡∏ö‡∏•‡∏≠‡∏´‡∏ô‡πà‡∏≠‡∏¢‡πÉ‡∏´‡πâ threshold ‡πÄ‡∏ô‡∏µ‡∏¢‡∏ô
    gray_blur = cv2.GaussianBlur(gray, (5, 5), 0)

    # ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç/‡∏ï‡∏±‡∏ß‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠‡∏™‡∏µ‡∏≠‡πà‡∏≠‡∏ô -> ‡πÉ‡∏ä‡πâ binary inverse
    # ‡∏õ‡∏£‡∏±‡∏ö 200 ‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏ß‡πà‡∏≤‡∏á‡πÑ‡∏î‡πâ
    _, th = cv2.threshold(gray_blur, 200, 255, cv2.THRESH_BINARY)

    # ‡∏õ‡∏¥‡∏î‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏•‡πá‡∏Å ‡πÜ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ï‡∏±‡∏ß‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏Å‡∏±‡∏ô
    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))
    th = cv2.dilate(th, kernel, iterations=1)
    th = cv2.erode(th, kernel, iterations=1)

    # ----- 3) ‡∏´‡∏≤ contour -----
    contours, _ = cv2.findContours(th, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    for cnt in contours:
        x, y, w, h = cv2.boundingRect(cnt)
        area = w * h

        # ‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡∏ô‡∏≤‡∏î‡∏Å‡∏•‡πà‡∏≠‡∏á (‡∏Å‡∏±‡∏ô noise ‡πÄ‡∏•‡πá‡∏Å ‡πÜ/‡πÉ‡∏´‡∏ç‡πà‡πÄ‡∏Å‡∏¥‡∏ô)
        if area < 50:      # ‡πÄ‡∏•‡πá‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ (‡∏à‡∏∏‡∏î noise)
            continue
        if area > 0.1 * (roi.shape[0] * roi.shape[1]):  # ‡πÉ‡∏´‡∏ç‡πà‡πÄ‡∏Å‡∏¥‡∏ô (‡∏ó‡∏±‡πâ‡∏á ROI) ‡∏ï‡∏±‡∏î‡∏ó‡∏¥‡πâ‡∏á
            continue

        # padding ‡∏£‡∏≠‡∏ö ‡πÜ ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢
        pad = 2
        x1 = max(0, roi_x1 + x - pad)
        y1 = max(0, roi_y1 + y - pad)
        x2 = min(W - 1, roi_x1 + x + w + pad)
        y2 = min(H - 1, roi_y1 + y + h + pad)

        # ‡∏•‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ï‡∏±‡∏ß‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠ (‡πÄ‡∏ï‡∏¥‡∏°‡∏î‡∏≥)
        cv2.rectangle(img, (x1, y1), (x2, y2), (0, 0, 0), -1)

    return img


# ---------- ‡∏™‡∏£‡πâ‡∏≤‡∏á model ----------
os.makedirs(SAVE_DIR, exist_ok=True)

detection_model = AutoDetectionModel.from_pretrained(
    model_type="ultralytics",
    model_path=DRONE_MODEL_PATH,
    confidence_threshold=CONF_THRESH,
    device="cpu",
)

print("‚úÖ SAHI + YOLO model ready")

print("\n=== ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏´‡∏°‡∏î‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô ===")
print("1: ‡πÄ‡∏ã‡∏ü‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏∏‡∏Å‡∏†‡∏≤‡∏û + ‡πÇ‡∏ä‡∏ß‡πå + ‡πÄ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ (Save All)")
print("2: ‡∏î‡∏π‡∏ó‡∏µ‡∏•‡∏∞‡∏†‡∏≤‡∏û ‡πÑ‡∏°‡πà‡πÄ‡∏ã‡∏ü (View Only)")
mode = input("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏´‡∏°‡∏î (1/2): ").strip()
save_all = (mode == "1")

if save_all:
    print(f"üíæ ‡πÇ‡∏´‡∏°‡∏î 1: ‡πÄ‡∏ã‡∏ü‡∏ó‡∏∏‡∏Å‡∏†‡∏≤‡∏û‡∏•‡∏á‡πÉ‡∏ô {SAVE_DIR}\n")
else:
    print("üëÅÔ∏è ‡πÇ‡∏´‡∏°‡∏î 2: ‡∏î‡∏π‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ‡πÑ‡∏°‡πà‡πÄ‡∏ã‡∏ü‡πÑ‡∏ü‡∏•‡πå\n")

image_files = sorted(
    f for f in os.listdir(TEST_DIR)
    if f.lower().endswith((".jpg", ".jpeg", ".png"))
)

for i, filename in enumerate(image_files, 1):
    img_path = os.path.join(TEST_DIR, filename)
    img = cv2.imread(img_path)
    if img is None:
        print(f"‡∏Ç‡πâ‡∏≤‡∏° {filename} (‡∏≠‡πà‡∏≤‡∏ô‡∏£‡∏π‡∏õ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à)")
        continue

    H, W = img.shape[:2]
    img_area = H * W
    ground_line = int(H * GROUND_RATIO)

    # ===== 1) ‡∏ó‡∏≥‡∏™‡∏≥‡πÄ‡∏ô‡∏≤‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö detection ‡πÅ‡∏•‡πâ‡∏ß‡∏•‡∏ö‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏ß‡∏•‡∏≤‡∏î‡πâ‡∏ß‡∏¢ contour =====
    img_for_det = img.copy()
    img_for_det = mask_datetime_by_contour(img_for_det)

    # ===== 2) ‡∏Ç‡∏¢‡∏≤‡∏¢‡∏†‡∏≤‡∏û‡∏Å‡πà‡∏≠‡∏ô detect =====
    img_zoom = cv2.resize(
        img_for_det, None, fx=ZOOM, fy=ZOOM,
        interpolation=cv2.INTER_LINEAR
    )

    # ===== 3) SAHI + YOLO slicing inference =====
    result = get_sliced_prediction(
        image=img_zoom,
        detection_model=detection_model,
        slice_height=int(SLICE_H * ZOOM),
        slice_width=int(SLICE_W * ZOOM),
        overlap_height_ratio=OVERLAP,
        overlap_width_ratio=OVERLAP,
    )

    drone_candidates = []

    for obj in result.object_prediction_list:
        class_name = obj.category.name
        score = float(obj.score.value)
        if class_name != DRONE_CLASS_NAME:
            continue

        zx1, zy1, zx2, zy2 = obj.bbox.to_xyxy()
        x1 = int(zx1 / ZOOM)
        y1 = int(zy1 / ZOOM)
        x2 = int(zx2 / ZOOM)
        y2 = int(zy2 / ZOOM)

        x1 = max(0, min(W - 1, x1))
        x2 = max(0, min(W - 1, x2))
        y1 = max(0, min(H - 1, y1))
        y2 = max(0, min(H - 1, y2))

        w = x2 - x1
        h = y2 - y1
        if w <= 0 or h <= 0:
            continue

        cx = (x1 + x2) // 2
        cy = (y1 + y2) // 2

        box_area = w * h
        if not (MIN_RATIO * img_area <= box_area <= MAX_RATIO * img_area):
            continue

        if (cy > ground_line) and (box_area > BIG_OBJ_RATIO * img_area):
            if (score < CONF_UNDER_LINE_THRESH):
                continue

        aspect1 = w / float(h)
        if aspect1 < 0.8:
            continue
        aspect2 = h / float(w)
        if aspect2 < 0.65:
            continue

        drone_candidates.append((score, x1, y1, x2, y2, cx, cy))

    drone_candidates.sort(key=lambda d: d[0], reverse=True)
    drone_candidates = drone_candidates[:MAX_DRONES]

    for score, x1, y1, x2, y2, cx, cy in drone_candidates:
        label = f"{DRONE_CLASS_NAME} {score:.2f}"
        cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)
        cv2.putText(img, label, (x1, y1 - 10),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
        print(f"[{i}/{len(image_files)}] {filename} -> {label} center=({cx},{cy})")

    if save_all:
        save_path = os.path.join(SAVE_DIR, filename)
        cv2.imwrite(save_path, img)
        print(f"üíæ Saved: {save_path}")

    # debug: ‡∏ß‡∏≤‡∏î ground line ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡πá‡∏ô
    cv2.line(img, (0, ground_line), (W-1, ground_line), (0, 0, 255), 2)

    img_disp = cv2.resize(img, (720, 480))
    cv2.imshow("Detect_Image", img_for_det)
    # cv2.imshow("Detect_Image", img_disp)

    if save_all:
        key = cv2.waitKey(AUTO_DELAY_MS) & 0xFF
        if key in [ord("q"), 27]:
            break
    else:
        print("‚û°Ô∏è  Space/Enter = ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ, q = ‡∏≠‡∏≠‡∏Å")
        key = cv2.waitKey(0) & 0xFF
        if key in [ord("q"), 27]:
            break

cv2.destroyAllWindows()
print("‚úÖ SAHI + YOLO (datetime contour mask) ‡πÄ‡∏™‡∏£‡πá‡∏à‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢")
